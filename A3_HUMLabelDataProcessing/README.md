This directory contains code written to extract and pre-process human-listener labelled data used in this study. 

Please note that only files with the prefix A1, A2, etc. are executables, while all other files are supporting functions. Please also note that the executable files need to be executed in the order of prefixes A1 to A6. Data required to execute all scripts in this directory as well as all shareable metadata files are provided in the associated OSF project (see main README for this repository for details).

`A1_IdOverlapAndFilesWErrors.m` identifies overlaps in vocalisations based on onsets and offsets as determined by human listeners, and chops up overlapping vocalisations into overlapping and non-overlapping sub-vocalisations. Requires `DetectOverlap.m` and `GetNonOverlapVocs.m`

` A2_TSAcousticsBatchProcessHUMlabel.m` extracts time series info from each .its file. Requires `getAcousticsTS_HUMlabel.m` and `getIndividualAudioSegmentsHUMlabel.m`. Note that a few files that are parts of day long recordings (with the suffix a, b, etc) don't find a wave file match, because the wave files aren't necessarily split up into a, b, etc. Make sure to check for this and do those manually if necessary. Also note that the acoustics info is computed for vocalisations that have been chopped up into overlapping and non-overlapping sub-vocalisation wherever applicable. 

`A3_AddingAnnotationToTS.m` adds human-listener annotation tags (T, U, N for adult vocalisation; and R, X, C, L for infant vocalisations) to the acoustics time series. This script also adds child speech-related (CHNSP) and child non-speech-related (CHNNSP) speaker labels to utterances that have been annotated as those of the infant vocalizing agent. Finally, this script reconstitutes the sub-vocalisations obtained from processing overlapping vocalisations (from `A1_IdOverlapAndFilesWErrors.m`) into the original vocalisations and computes estimates of mean pitch and amplitude for the re-constituted vocalisations based on the fraction of the full vocalisation's duration occupied by each sub-vocalisation for which acoustics have been computed (This is a little hard to explain without the code, so please make sure to check the script. A schematic describing how the overlaps are processed is shown in `HumLabelOlpProcessingSchematic.pdf`). This script requires ` StitchBackOlpProcessedVocs.m`.
 
` A4_MergeSubRecs_GetSectionNum.m ` merges any sub-recordings that are from the same infant on the same day and assigns section numbers to each utterance. Further, this script:
1.	flags files which have 5-minute sections that were marked in the coding spreadsheet for human listener annotation but were not annotated. This info is written into table `FilesWithUnannotatedSections.csv`
2.	flags files which have 5 minute sections that are less than 30 minutes apart (excluding 5-minute sections that are from different sub-recordings). This info is written into table `FilesWithLessThan30minBnSections.csv`.

`5_GetZscoredData_LENAandHumLabels.m` takes the acoustics (pitch, amplitude, duration) from the entire dataset (both LENA and human-listener labelled data), log transforms duration and mean pitch, z-scores the data, and saves the z-scored acoustics data and other time series data for both LENA and human-listener labelled data. Please note that this means that the z-scoring is done wrt the combined LENA- and human-listener labelled data, since the acoustics for both were estimated independently. This also means that the z-scored LENA data is *only* generated at this step. 

`A6_MatchingSectionsToCodingSheet.m` matches the human-listener annotated 5 minute sections to the corresponding 5 minute sections in the LENA-labelled data, and saves these data tables, to facilitate validation.

For more specific details, please read comments about paths and other notes in the files before executing them.
