This directory contains code written to extract and pre-process human-listener labelled data used in this study. 

Please note that only files with the prefix A1, A2, etc. are executables, while all other files are supporting functions. Please also note that the executable files need to be executed in the order of prefixes A1 to A6. Data required to execute all scripts in this directory as well as all relevant metadata files are provided in the OSF repository associated with this project (see main README for this repository for details). For details on output files for each executable script as well as data organisation, see `Metadata_CodeAndFiles.xlsx` and `README_FileAndDirectoryDetails.docx` (available in this repository as well as in the associated OSF repository).

`A1_IdOverlapAndFilesWErrors.m` identifies overlaps in vocalisations based on onsets and offsets as determined by human listeners, and chops up overlapping vocalisations into overlapping and non-overlapping sub-vocalisations. Requires `DetectOverlap.m` and `GetNonOverlapVocs.m`. Schematics describing overlap processing are available in `OlpProcSchem.pptx`.

`A2_TSAcousticsBatchProcessHUMlabel.m` extracts time series info from each .its file. Requires `getPraatAcoustics.m`, `getAcousticsTS_HUMlabel.m`, and `getIndividualAudioSegmentsHUMlabel.m`. Note that a few files that come from different sub-recordings within the same daylong recording (identified by suffixes a, b, etc.) don't have a wave file match, because the wave files aren't necessarily split up into a, b, etc. While the script accounts for this, it is advisable to check for such exceptions and write additional code to deal with these exceptions (if they occur) or to do the time series processing manually for those files. Acoustics info is computed for vocalisations that have been chopped up into overlapping and non-overlapping sub-vocalisations wherever applicable. 

`A3_AddAnnotationToTS_AndRemoveOLP.m` adds human-listener annotation tags (T, U, N for adult vocalisation; and R, X, C, L for infant vocalisations) back to the acoustics time series. This script also adds child speech-related (referred to as CHNSP in the code and ChSp in the Burstiness pre-print (2025) and paper) and child non-speech-related (referred to as CHNNSP in the code and ChNsp in the Burstiness pre-print (2025) and paper) speaker labels to infant utterances. Finally, this script removes sub-vocalisations (obtained from processing overlapping vocalisations in `A1_IdOverlapAndFilesWErrors.m`) that are tagged as overlaps.
 
`A4_MergeSubRecs_GetSectionNum.m` merges files from sub-recordings that are from the same daylong recording and assigns section numbers to each utterance. Further, this script:
-	flags files which have 5-minute sections that were marked in the coding spreadsheet for human listener annotation but were not annotated. This info is written into the table `FilesWithUnannotatedSections.csv`
-	flags files which have 5 minute sections that are less than 30 minutes apart (excluding 5-minute sections that are from different sub-recordings). This info is written into table `FilesWithLessThan30minBnSections.csv`

`A5_GetZscoredData_LENAandHumLabels.m` takes the acoustics (pitch, amplitude, duration) from the entire dataset (both LENA and human-listener labelled data), log transforms duration and mean pitch, z-scores the data, and saves the z-scored acoustics data and other time series data for both LENA and human-listener labelled data. Z-scoring is done wrt the combined LENA- and human-listener labelled data. Note that the z-scored LENA acoustics and duration data are *only* generated at this step. Also note that the Burstiness paper and pre-print (2025) does not present results based on acoustics or duration data (which are the z-scored variables). Nevertheless, this is part of the data processing pipeline and this code is presented here as part of the codebase. 

`A6_MatchingSectionsToCodingSheet.m` matches the human-listener annotated 5 minute sections to the corresponding 5 minute sections in the LENA-labelled data, and saves these data tables, to facilitate validation.

`A7_GetMetadataForValidationData.m` generates a metadata file (`ValDataMergedTSMetaDataTab.csv`) for the human listener-labelled data.

For more specific details, please read comments about paths and other notes in the files before executing them.
