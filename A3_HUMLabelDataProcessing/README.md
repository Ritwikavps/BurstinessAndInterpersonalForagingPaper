This directory contains code written to extract and pre-process human-listener labelled data used in this study. 

Please note that only files with the prefix A1, A2, etc. are executables, while all other files are supporting functions. Please also note that the executable files need to be executed in the order of prefixes A1 to A6. Data required to execute all scripts in this directory as well as all shareable metadata files are provided in the OSF repository associated with this project (see main README for this repository for details). For details on data organisation and what data is available/not available publicly, see the OSF repository associated with this project. For details on output files for each executable script (and which of those output files are shared publicly), see `Metadata_CodeAndFiles.xlsx` in the main repository.

`A1_IdOverlapAndFilesWErrors.m` identifies overlaps in vocalisations based on onsets and offsets as determined by human listeners, and chops up overlapping vocalisations into overlapping and non-overlapping sub-vocalisations. Requires `DetectOverlap.m` and `GetNonOverlapVocs.m`. Schematics describing overlap processing are available in `OlpProcSchem.pptx`.

`A2_TSAcousticsBatchProcessHUMlabel.m` extracts time series info from each .its file. Requires `getPraatAcoustics.m`, `getAcousticsTS_HUMlabel.m`, and `getIndividualAudioSegmentsHUMlabel.m`. Note that a few files that come from different sub-recordings within the same daylong recording (identified by suffixes a, b, etc.) don't have a wave file match, because the wave files aren't necessarily split up into a, b, etc. While the script accounts for this, it is advisable to check for such exceptions and write additional code to deal with these exceptions (if they occur) or to do the time series processing manually for those files. Acoustics info is computed for vocalisations that have been chopped up into overlapping and non-overlapping sub-vocalisation wherever applicable. 

`A3_AddAnnotationToTS_AndRemoveOLP.m` adds human-listener annotation tags (T, U, N for adult vocalisation; and R, X, C, L for infant vocalisations) back to the acoustics time series. This script also adds child speech-related (CHNSP) and child non-speech-related (CHNNSP) speaker labels to infant utterances. Finally, this script removes sub-vocalisations (obtained from processing overlapping vocalisations in `A1_IdOverlapAndFilesWErrors.m`) that are tagged as overlaps.
 
`A4_MergeSubRecs_GetSectionNum.m` merges files from sub-recordings that are from the same daylong recording and assigns section numbers to each utterance. Further, this script:
1.	flags files which have 5-minute sections that were marked in the coding spreadsheet for human listener annotation but were not annotated. This info is written into table `FilesWithUnannotatedSections.csv`
2.	flags files which have 5 minute sections that are less than 30 minutes apart (excluding 5-minute sections that are from different sub-recordings). This info is written into table `FilesWithLessThan30minBnSections.csv`.

`5_GetZscoredData_LENAandHumLabels.m` takes the acoustics (pitch, amplitude, duration) from the entire dataset (both LENA and human-listener labelled data), log transforms duration and mean pitch, z-scores the data, and saves the z-scored acoustics data and other time series data for both LENA and human-listener labelled data. Please note that this means that the z-scoring is done wrt the combined LENA- and human-listener labelled data, since the acoustics for both were estimated independently. This also means that the z-scored LENA data is *only* generated at this step. 

The z-scoring of the combined LENA and human-listener data is done after extracting human-listener labelled data (the code for which is provided in `A3_HUMLabelDataProcessing`). Note that the associated paper does not present results based on acoustics or duration data (which are the z-scored variables). Nevertheless, this is part of the data processing pipeline and this code is presented here as part of the codebase. 

`A6_MatchingSectionsToCodingSheet.m` matches the human-listener annotated 5 minute sections to the corresponding 5 minute sections in the LENA-labelled data, and saves these data tables, to facilitate validation.

For more specific details, please read comments about paths and other notes in the files before executing them.
